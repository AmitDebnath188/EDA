{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c312baf",
   "metadata": {},
   "source": [
    "# LIBRARY IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "907ff231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "from warnings import warn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "import time\n",
    "import warnings \n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0140e",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58012921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the local CSV file using pandas\n",
    "data = pd.read_csv(r\"C:\\Users\\AmitDebnath\\Downloads\\Kaggle dataset and ground truth\\p20_1280_rows_value.csv\")\n",
    "\n",
    "# Extract the single column as 1D array\n",
    "raw_eda_full = data.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91affc2",
   "metadata": {},
   "source": [
    "# SET SAMPLE RATE AND START AND END POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f8b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Prepare for the iterative analysis ---\n",
    "all_results = []\n",
    "sampling_rate = 128\n",
    "end_point = 1260"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5a947",
   "metadata": {},
   "source": [
    "# MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bac22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Loop through the data with an incrementing start point ---\n",
    "for start_point in range(end_point):\n",
    "    # Slice the raw EDA data for the current window.\n",
    "    raw_eda_window = raw_eda_full[start_point:end_point]\n",
    "\n",
    "    print(f\"--- Processing slice: {start_point} to {end_point} ---\")\n",
    "    \n",
    "    # --- 4. Process the sliced signal ---\n",
    "    try:\n",
    "        # We will catch and ignore the specific RuntimeWarning that occurs when no peaks are found.\n",
    "        # This is expected behavior for short signal windows.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "            signals, info = nk.eda_process(raw_eda_window, sampling_rate=sampling_rate, method=\"neurokit\")\n",
    "            analysis_df = nk.eda_intervalrelated(signals, sampling_rate=sampling_rate)\n",
    "\n",
    "        # Add identifiers for the data slice\n",
    "        analysis_df['start_index'] = start_point\n",
    "        analysis_df['end_index'] = end_point\n",
    "\n",
    "        # --- 6. Calculate additional features from 'info' --- for SCR peak related features\n",
    "        peak_amplitudes = signals[\"SCR_Amplitude\"]\n",
    "        \n",
    "        # For derivative features, ALSC, INSC, APSC, RMSC\n",
    "        clean = signals[\"EDA_Clean\"]\n",
    "        \n",
    "        eda_phasic = signals[\"EDA_Phasic\"]\n",
    "        \n",
    "        # For peaks related features\n",
    "        peaks = signals[\"SCR_Amplitude\"]\n",
    "        \n",
    "        US_TO_S_CONVERSION = 1_000_000.0\n",
    "        \n",
    "        \n",
    "        # 1. This check correctly where peaks were found\n",
    "        analysis_df['MEAN'] = np.nanmean(clean) / US_TO_S_CONVERSION\n",
    "        analysis_df['MAX'] = np.nanmax(clean) / US_TO_S_CONVERSION\n",
    "        analysis_df['MIN'] = np.nanmin(clean) / US_TO_S_CONVERSION\n",
    "        analysis_df['RANGE'] = (analysis_df['MAX'] - analysis_df['MIN']) / US_TO_S_CONVERSION\n",
    "        analysis_df['SKEWNESS'] = pd.Series(clean).skew()\n",
    "        analysis_df['KURTOSIS'] = pd.Series(clean).kurtosis()\n",
    "\n",
    "\n",
    "        # --- 2. Calculate derivative Features ---\n",
    "        # --- 2. Calculate derivative/Gradient Features ---\n",
    "        first_derivative = np.gradient(eda_phasic) \n",
    "        second_derivative = np.gradient(first_derivative)\n",
    "\n",
    "        analysis_df['MEAN_D1'] = np.mean(first_derivative)\n",
    "        analysis_df['STD_D1'] = np.std(first_derivative)\n",
    "        analysis_df['MEAN_D2'] = np.mean(second_derivative)\n",
    "        analysis_df['STD_D2'] = np.std(second_derivative)\n",
    "        \n",
    "        # 3. Find out the ALSC- Arc length of the scr (EDA_phasic)\n",
    "        t = np.arange(len(eda_phasic)) / sampling_rate  # Create a time array\n",
    "        analysis_df['ALSC'] = np.sum(np.sqrt(np.diff(t) ** 2 + np.diff(eda_phasic) ** 2))\n",
    "        \n",
    "        # 4. INSC - Integral of the scr (EDA_phasic)\n",
    "        dt = 1 / sampling_rate  # Time difference between samples\n",
    "        \n",
    "        # Integral calculations for scr (EDA_Phasic)\n",
    "        analysis_df['INSC'] = np.sum(eda_phasic) * dt  # Simple Riemann sum approximation\n",
    "        \n",
    "        # Use trapezoidal rule for better accuracy (better for uneven signals)\n",
    "        analysis_df['INSC_trapz'] = np.trapezoid(eda_phasic, dx=dt)\n",
    "        \n",
    "        # 5. APSC - Average Power of the scr (EDA_phasic)\n",
    "        avg_power = np.mean(eda_phasic**2)\n",
    "        analysis_df['APSC'] = avg_power / np.max(eda_phasic**2) if np.max(eda_phasic**2) != 0 else 0\n",
    "        \n",
    "        # 6. RMSC - Root Mean Square of the scr (EDA_phasic)\n",
    "        rms = np.sqrt(np.mean(eda_phasic**2))\n",
    "        \n",
    "        max_rms = np.max(np.abs(eda_phasic)) if np.max(np.abs(eda_phasic)) != 0 else 0\n",
    "        \n",
    "        analysis_df['RMSC'] = rms / max_rms if max_rms != 0 else 0\n",
    "        \n",
    "        onsets = signals['EDA_Clean'][signals[\"SCR_Onsets\"] == 1]\n",
    "        \n",
    "        # 7. SCR onsets\n",
    "        analysis_df['MEAN_Onsets'] = np.nanmean(onsets)\n",
    "        analysis_df['MAX_Onsets'] = np.nanmax(onsets)\n",
    "        analysis_df['MIN_Onsets'] = np.nanmin(onsets)\n",
    "        analysis_df['STD_Onsets'] = np.nanstd(onsets)\n",
    "        \n",
    "        # 8. SCR Peaks\n",
    "        analysis_df['MEAN_Peaks'] = np.nanmean(peaks)\n",
    "        analysis_df['MAX_Peaks'] = np.nanmax(peaks)\n",
    "        analysis_df['MIN_Peaks'] = np.nanmin(peaks)\n",
    "        analysis_df['STD_Peaks'] = np.nanstd(peaks)\n",
    "\n",
    "        # Append the results of this iteration\n",
    "        all_results.append(analysis_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing at iteration {start_point}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72430d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Combine all results and save to CSV ---\n",
    "if all_results:\n",
    "    final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    # Neurokit sometimes outputs NaN for mean amplitude when no peaks are found.\n",
    "    # It's good practice to fill these with 0 to maintain consistency.\n",
    "    final_results_df.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Final DataFrame with All Features from All Iterations ---\")\n",
    "    print(final_results_df)\n",
    "\n",
    "    final_results_df.to_csv(\"C:/Users/AmitDebnath/Downloads/eda_analysis_results_p20-1280_updated_values_9th_thursday_for_derivative_features.csv\", index=False)\n",
    "    print(\"\\nResults have been successfully saved to 'eda_analysis_results_fixed.csv'\")\n",
    "else:\n",
    "    print(\"\\nNo results were generated to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
